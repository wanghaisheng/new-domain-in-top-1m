name: Test Import Tools

on:
  workflow_dispatch:  # Manual trigger
  push:
    paths:
      - 'import_historical_data_chunked.py'
      - 'run_chunked_import.py'
      - 'commit_results.py'

jobs:
  test_import_tools:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 psutil gitpython pandas pyarrow
      
      - name: Run test workflow
        run: |
          # Create test script
          cat > test_workflow.sh << 'EOF'
          #!/bin/bash
          
          echo "=== Testing Chunked Historical Data Import Tools ==="
          echo ""
          
          # Step 1: Prepare a small list of commits for testing
          echo "Step 1: Preparing commit list with a small date range..."
          python prepare_commits.py --start-date 2024-06-08 --end-date 2024-06-10
          echo ""
          
          # Step 2: Create test directory structure
          echo "Step 2: Creating test directory structure..."
          mkdir -p historical_extracts/2024-06-08
          mkdir -p historical_extracts/2024-06-09
          mkdir -p historical_extracts/2024-06-10
          
          # Create dummy data files
          echo "1,example.com" > historical_extracts/2024-06-08/top-1m.csv
          echo "1,example.org" > historical_extracts/2024-06-09/top-1m.csv
          echo "1,example.net" > historical_extracts/2024-06-10/top-1m.csv
          echo ""
          
          # Step 3: Process a single chunk
          echo "Step 3: Processing a single chunk..."
          python import_historical_data_chunked.py --chunk-id 1 --batch-size 1000
          echo ""
          
          # Step 4: Merge results
          echo "Step 4: Merging results..."
          python import_historical_data_chunked.py --merge-only
          echo ""
          
          # Step 5: Commit results
          echo "Step 5: Committing results (dry run)..."
          # Just check if the script runs without errors
          if [ -f "commit_results.py" ]; then
            echo "commit_results.py exists, would run in production"
          else
            echo "commit_results.py not found"
          fi
          echo ""
          
          # Step 6: Verify results
          echo "Step 6: Verifying results..."
          echo "Checking for output files:"
          
          if [ -f "domains_rankings.parquet" ]; then
              echo "- domains_rankings.parquet: FOUND"
              python -c "import pandas as pd; df = pd.read_parquet('domains_rankings.parquet'); print(f'  Rows: {len(df)}')"
          else
              echo "- domains_rankings.parquet: NOT FOUND"
          fi
          
          if [ -f "domains_first_seen.parquet" ]; then
              echo "- domains_first_seen.parquet: FOUND"
              python -c "import pandas as pd; df = pd.read_parquet('domains_first_seen.parquet'); print(f'  Rows: {len(df)}')"
          else
              echo "- domains_first_seen.parquet: NOT FOUND"
          fi
          
          if [ -f "import_checkpoint.json" ]; then
              echo "- import_checkpoint.json: FOUND"
              echo "  Checkpoint contents:"
              cat "import_checkpoint.json"
          else
              echo "- import_checkpoint.json: NOT FOUND"
          fi
          
          echo ""
          echo "=== Test Complete ==="
          EOF
          
          # Make script executable
          chmod +x test_workflow.sh
          
          # Run the test script
          ./test_workflow.sh
      
      - name: Push artifacts
        uses: ad-m/github-push-action@master
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          branch: main